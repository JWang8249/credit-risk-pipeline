# Credit Risk Prediction Pipeline

A complete and reproducible **Machine Learning pipeline** for predicting credit default risk â€” including **data preprocessing**, **model training**, **explainability with SHAP**, **FastAPI backend**, **Streamlit frontend**, and **PostgreSQL persistence**, all containerized via **Docker** and **automated with CI/CD**.

---

## ğŸ‘¤ Author

**Jingyi Wang**  
Masterâ€™s Student in *Data Science and Society (Business Track)*  
Tilburg University, The Netherlands  
Version: **v2.0.0** (2025-10)  
Status: âœ… Fully reproducible and containerized

---

## ğŸ§© Overview

This project demonstrates a **modular and reproducible ML workflow**:

1. ğŸ“Š **Data preprocessing** (`src/data_preprocess.py`)  
2. ğŸ¤– **Model training** (`src/model_train.py`)  
3. ğŸ“ˆ **Evaluation + Explainability** (SHAP visualization)  
4. âš™ï¸ **FastAPI REST service** for predictions  
5. ğŸ–¥ï¸ **Streamlit web app** for interactive user input  
6. ğŸ—„ï¸ **PostgreSQL** for saving predictions  
7. ğŸ¤® **Pytest** for automated testing  
8. ğŸ³ **Docker & Docker Compose** for environment reproducibility  
9. ğŸš¦ **CI/CD pipeline** with GitHub Actions

---

## âš™ï¸ Project Structure

```
credit-risk-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/credit_data_clean.csv
â”‚   â”œâ”€â”€ raw/credit_data.csv
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ shap_summary.png
â”‚   â”œâ”€â”€ shap_feature_importance.png
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ model.pkl
â”‚   â”œâ”€â”€ scaler.pkl
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ api.py
â”‚   â”œâ”€â”€ app.py
â”‚   â”œâ”€â”€ data_preprocess.py
â”‚   â”œâ”€â”€ model_train.py
â”‚   â”œâ”€â”€ model_eval.py
â”‚   â””â”€â”€ test_model.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Makefile
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ How to Run Locally

### 1ï¸âƒ£ Prepare Environment
```bash
conda create -n creditrisk python=3.10
conda activate creditrisk
pip install -r requirements.txt
```

### 2ï¸âƒ£ Preprocess Data
```bash
python src/data_preprocess.py
```

### 3ï¸âƒ£ Train Model
```bash
make train
```

### 4ï¸âƒ£ Evaluate with SHAP
```bash
make eval
```
Generated visuals:
- `docs/shap_summary.png`
- `docs/shap_feature_importance.png`

### 5ï¸âƒ£ Run API
```bash
make serve
```
Visit ğŸ”— http://127.0.0.1:8000/docs

### 6ï¸âƒ£ Run Streamlit App
```bash
make app
```
Visit ğŸ”— http://127.0.0.1:8501

---

## ğŸ³ Run with Docker

### 1ï¸âƒ£ Build Image
```bash
docker build -t creditrisk-api .
```

### 2ï¸âƒ£ Run API
```bash
docker run -p 8000:8000 creditrisk-api
```

### 3ï¸âƒ£ Test Endpoint
```bash
curl -X POST "http://127.0.0.1:8000/predict" \
-H "Content-Type: application/json" \
-d "{\"LIMIT_BAL\":20000, \"SEX\":1, \"EDUCATION\":2, \"MARRIAGE\":1, \"AGE\":35}"
```

---

## ğŸ§± docker-compose: Multi-Container Setup

Run the full stack (API + Streamlit + PostgreSQL):

```bash
docker-compose up -d
```

Services:
| Service | Port | Description |
|----------|------|-------------|
| `creditrisk-db` | 5432 | PostgreSQL database |
| `creditrisk-api` | 8000 | FastAPI model server |
| `creditrisk-app` | 8501 | Streamlit frontend |

Then open:
- http://127.0.0.1:8000/docs â†’ FastAPI
- http://127.0.0.1:8501 â†’ Streamlit App

---

## ğŸ¤“ SHAP Explainability

The pipeline integrates **SHAP (SHapley Additive exPlanations)** to interpret model predictions.

### Example Output:
| File | Description |
|------|--------------|
| `docs/shap_summary.png` | Feature importance summary |
| `docs/shap_feature_importance.png` | Bar plot of top drivers |

```python
import shap
explainer = shap.Explainer(model, X_scaled)
shap_values = explainer(X_scaled)
shap.summary_plot(shap_values, X, show=False)
```

---

## ğŸ”„ CI/CD Pipeline (GitHub Actions)

File: `.github/workflows/ci.yml`

### CI Stages:
1. **Checkout repo**
2. **Set up Python environment**
3. **Install dependencies**
4. **Run unit tests (pytest)**
5. **Build Docker image**

Badge example:
[![CI](https://github.com/tcsai/credit-risk-pipeline/actions/workflows/ci.yml/badge.svg)](https://github.com/tcsai/credit-risk-pipeline/actions/workflows/ci.yml)

This ensures every commit is:
- âœ… Lint-checked
- âœ… Tested
- âœ… Docker-buildable

---

## ğŸ¤ª Testing

Run automated tests:
```bash
make test
```

Tests include:
- Model loading  
- Prediction correctness  
- API endpoint validation  
- Database integration  

---

## ğŸ“Š Example Prediction

Input:
```json
{
  "LIMIT_BAL": 20000,
  "SEX": 1,
  "EDUCATION": 2,
  "MARRIAGE": 1,
  "AGE": 35
}
```

Output:
```json
{
  "prediction": 0,
  "risk": "Low Risk"
}
```

All predictions are logged in PostgreSQL table `predictions`.

---

## ğŸ“¦ Makefile Commands

| Command | Description |
|----------|--------------|
| `make train` | Train the model |
| `make eval` | Evaluate and generate SHAP |
| `make serve` | Start FastAPI server |
| `make app` | Run Streamlit frontend |
| `make test` | Run all pytest scripts |
| `make all` | Run full pipeline (train â†’ eval â†’ test) |

---

## ğŸ§  Key Learnings

- Building reproducible ML pipelines requires clear separation between **training**, **serving**, and **testing**.  
- Docker + CI/CD ensures deterministic builds.  
- SHAP provides interpretable insights essential for **Law + AI** applications.  

---

## ğŸ“œ License

Licensed under the **MIT License**.  
Feel free to fork and adapt for your own data science portfolio.

---